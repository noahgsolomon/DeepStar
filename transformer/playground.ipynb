{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import tiktoken\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bert(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoding = tiktoken.get_encoding(\"r50k_base\")\n",
    "        self.emb_size = self.encoding.n_vocab\n",
    "        self.emb_channels = 128\n",
    "        self.max_token_length = 512\n",
    "        self.emb = nn.Embedding(self.emb_size, self.emb_channels)\n",
    "        self.positional_encoding = nn.Parameter(torch.randn(self.max_token_length, self.emb_channels))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 7.81k/7.81k [00:00<00:00, 20.3MB/s]\n",
      "Downloading data: 100%|██████████| 21.0M/21.0M [00:01<00:00, 16.6MB/s]\n",
      "Downloading data: 100%|██████████| 20.5M/20.5M [00:00<00:00, 22.6MB/s]\n",
      "Downloading data: 100%|██████████| 42.0M/42.0M [00:02<00:00, 18.7MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:04<00:00,  1.50s/it]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 1045.70it/s]\n",
      "Generating train split: 100%|██████████| 25000/25000 [00:00<00:00, 282663.99 examples/s]\n",
      "Generating test split: 100%|██████████| 25000/25000 [00:00<00:00, 399801.73 examples/s]\n",
      "Generating unsupervised split: 100%|██████████| 50000/50000 [00:00<00:00, 435954.83 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('imdb', split='train[:1%]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.', 'label': 0}\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "tokenized_input = tokenizer(dataset['text'], padding=True, truncation=True, max_length=512, return_tensors='pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.sep_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [CLS] i rented i am curious - yellow from my video store because of all the controversy that surrounded it when it was first released in 1967 . i also heard that at first it was seized by u . s . customs if it ever tried to enter this country , therefore being a fan of films considered \" controversial \" i really had to see this for myself . < br / > < br / > the plot is centered around a young swedish drama student named lena who wants to learn everything she can about life . in particular she wants to focus her attention ##s to making some sort of documentary on what the average sw ##ede thought about certain political issues such as the vietnam war and race issues in the united states . in between asking politicians and ordinary den ##ize ##ns of stockholm about their opinions on politics , she has sex with her drama teacher , classmates , and married men . < br / > < br / > what kills me about i am curious - yellow is that 40 years ago , this was considered pornographic . really , the sex and nu ##dity scenes are few and far between , even then it ' s not shot like some cheap ##ly made porn ##o . while my country ##men mind find it shocking , in reality sex and nu ##dity are a major staple in swedish cinema . even ing ##mar bergman , arguably their answer to good old boy john ford , had sex scenes in his films . < br / > < br / > i do com ##men ##d the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in america . i am curious - yellow is a good film for anyone wanting to study the meat and potatoes ( no pun intended ) of swedish cinema . but really , this film doesn ' t have much of a plot . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "string = ''\n",
    "for t in tokenized_input.input_ids[0]:\n",
    "    string += ' ' + tokenizer.convert_ids_to_tokens(t.item())\n",
    "\n",
    "print(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  101,  1045, 12524,  1045,  2572,  8025,  1011,  3756,  2013,  2026,\n",
       "         2678,  3573,  2138,  1997,  2035,  1996,  6704,  2008,  5129,  2009,\n",
       "         2043,  2009,  2001,  2034,  2207,  1999,  3476,  1012,  1045,  2036,\n",
       "         2657,  2008,  2012,  2034,  2009,  2001,  8243,  2011,  1057,  1012,\n",
       "         1055,  1012,  8205,  2065,  2009,  2412,  2699,  2000,  4607,  2023,\n",
       "         2406,  1010,  3568,  2108,  1037,  5470,  1997,  3152,  2641,  1000,\n",
       "         6801,  1000,  1045,  2428,  2018,  2000,  2156,  2023,  2005,  2870,\n",
       "         1012,  1026,  7987,  1013,  1028,  1026,  7987,  1013,  1028,  1996,\n",
       "         5436,  2003,  8857,  2105,  1037,  2402,  4467,  3689,  3076,  2315,\n",
       "        14229,  2040,  4122,  2000,  4553,  2673,  2016,  2064,  2055,  2166,\n",
       "         1012,  1999,  3327,  2016,  4122,  2000,  3579,  2014,  3086,  2015,\n",
       "         2000,  2437,  2070,  4066,  1997,  4516,  2006,  2054,  1996,  2779,\n",
       "        25430, 14728,  2245,  2055,  3056,  2576,  3314,  2107,  2004,  1996,\n",
       "         5148,  2162,  1998,  2679,  3314,  1999,  1996,  2142,  2163,  1012,\n",
       "         1999,  2090,  4851,  8801,  1998,  6623,  7939,  4697,  3619,  1997,\n",
       "         8947,  2055,  2037, 10740,  2006,  4331,  1010,  2016,  2038,  3348,\n",
       "         2007,  2014,  3689,  3836,  1010, 19846,  1010,  1998,  2496,  2273,\n",
       "         1012,  1026,  7987,  1013,  1028,  1026,  7987,  1013,  1028,  2054,\n",
       "         8563,  2033,  2055,  1045,  2572,  8025,  1011,  3756,  2003,  2008,\n",
       "         2871,  2086,  3283,  1010,  2023,  2001,  2641, 26932,  1012,  2428,\n",
       "         1010,  1996,  3348,  1998, 16371, 25469,  5019,  2024,  2261,  1998,\n",
       "         2521,  2090,  1010,  2130,  2059,  2009,  1005,  1055,  2025,  2915,\n",
       "         2066,  2070, 10036,  2135,  2081, 22555,  2080,  1012,  2096,  2026,\n",
       "         2406,  3549,  2568,  2424,  2009, 16880,  1010,  1999,  4507,  3348,\n",
       "         1998, 16371, 25469,  2024,  1037,  2350, 18785,  1999,  4467,  5988,\n",
       "         1012,  2130, 13749,  7849, 24544,  1010, 15835,  2037,  3437,  2000,\n",
       "         2204,  2214,  2879,  2198,  4811,  1010,  2018,  3348,  5019,  1999,\n",
       "         2010,  3152,  1012,  1026,  7987,  1013,  1028,  1026,  7987,  1013,\n",
       "         1028,  1045,  2079,  4012,  3549,  2094,  1996, 16587,  2005,  1996,\n",
       "         2755,  2008,  2151,  3348,  3491,  1999,  1996,  2143,  2003,  3491,\n",
       "         2005,  6018,  5682,  2738,  2084,  2074,  2000,  5213,  2111,  1998,\n",
       "         2191,  2769,  2000,  2022,  3491,  1999, 26932, 12370,  1999,  2637,\n",
       "         1012,  1045,  2572,  8025,  1011,  3756,  2003,  1037,  2204,  2143,\n",
       "         2005,  3087,  5782,  2000,  2817,  1996,  6240,  1998, 14629,  1006,\n",
       "         2053, 26136,  3832,  1007,  1997,  4467,  5988,  1012,  2021,  2428,\n",
       "         1010,  2023,  2143,  2987,  1005,  1056,  2031,  2172,  1997,  1037,\n",
       "         5436,  1012,   102,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train has 301,966 tokens\n",
      "val has 36,059 tokens\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "\n",
    "# download the tiny shakespeare dataset\n",
    "input_file_path = os.path.join('', 'input.txt')\n",
    "if not os.path.exists(input_file_path):\n",
    "    data_url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
    "    with open(input_file_path, 'w') as f:\n",
    "        f.write(requests.get(data_url).text)\n",
    "\n",
    "with open(input_file_path, 'r') as f:\n",
    "    data = f.read()\n",
    "n = len(data)\n",
    "train_data = data[:int(n*0.9)]\n",
    "val_data = data[int(n*0.9):]\n",
    "\n",
    "# encode with tiktoken gpt2 bpe\n",
    "enc = tiktoken.get_encoding(\"r50k_base\")\n",
    "train_ids = enc.encode_ordinary(train_data)\n",
    "val_ids = enc.encode_ordinary(val_data)\n",
    "print(f\"train has {len(train_ids):,} tokens\")\n",
    "print(f\"val has {len(val_ids):,} tokens\")\n",
    "\n",
    "# export to bin files\n",
    "train_ids = np.array(train_ids, dtype=np.int32)\n",
    "val_ids = np.array(val_ids, dtype=np.int32)\n",
    "train_ids.tofile(os.path.join('', 'train.bin'))\n",
    "val_ids.tofile(os.path.join('', 'val.bin'))\n",
    "\n",
    "# train.bin has 301,966 tokens\n",
    "# val.bin has 36,059 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/noahs/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/noahs/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DecoderTransformer(\n",
       "  (emb): Embedding(50257, 128)\n",
       "  (l1): Linear(in_features=128, out_features=250, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (l2): Linear(in_features=250, out_features=128, bias=True)\n",
       "  (ln1): LayerNorm()\n",
       "  (ln2): LayerNorm()\n",
       "  (linear): Linear(in_features=128, out_features=50257, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "from decoder import DecoderTransformer\n",
    "from token_dataset import TokenDataset\n",
    "\n",
    "# Parameters\n",
    "seq_length = 100\n",
    "batch_size = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Dataset and DataLoader\n",
    "train_dataset = TokenDataset('train.bin', seq_length)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Model, Loss Function, Optimizer\n",
    "model = DecoderTransformer()\n",
    "crossentropy = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 8.456459999084473, count: 14:   0%|          | 0/1 [00:07<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Backward and optimize\u001b[39;00m\n\u001b[1;32m     14\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 15\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     17\u001b[0m count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import trange\n",
    "num_epochs = 5  # Number of epochs\n",
    "\n",
    "for epoch in (t:=trange(1)):\n",
    "    count = 0\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        outputs = outputs.view(-1, outputs.size(-1))\n",
    "        loss = crossentropy(outputs, targets.view(-1))\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        count += 1\n",
    "        t.set_description(f'loss: {loss.item()}, count: {count}')\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And for butUS thee,First hereCOR in3\n",
      ", truthOL talkTell wild aUN\n",
      " dishon\n",
      " dayW I Dor thatrieve up nobleUS ofORK manWhyHow thyfather\n",
      " Experts ac:Y talk,Yet H\n",
      " name? singular slave one noble thisgo me lie him:\n",
      ", is requestodes ThomasAulet long ages; stand?\n",
      " in lord I heHAM\n",
      " late! ab kneeUS you as you\n",
      "I doTh it her now. lie hereRAY\n",
      ".. fri that Tokens Sir partWAR it you shallHoldforce will\n",
      "\n",
      " majestyW is\n",
      " as. am for\n",
      ",ERrimge men\n",
      " bout another forslaveEW be th. come I. more: much Rain,One amKING '\n",
      ": shortrown,You\n",
      " look myix alas!\n",
      " good them not might, their been of most\n",
      " way kingAs, Flu. youngerTeX\n",
      "IA's be toRInt\n",
      "\n",
      "\n",
      " my but playersO markComeLAND:Which in. me boastENable inBKING my\n",
      "\n",
      " anJ\n",
      " to you straight's deatharest it the with heThe, on\n",
      " was,\n",
      " cannot\n",
      " London officeIO most hand his maid so our A\n",
      " confSLIX than notcats chastIlicts for\n",
      " betweenous.A;My are nurse\n",
      "\n",
      " thyheadUD. such,AR:\n",
      " defend t qu my hence ages-\n",
      " that\n",
      " dear byOW mischiefMER lo.,,VOL the comfort mighty\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "input_text = \" \"\n",
    "\n",
    "input_ids = model.encoding.encode(input_text)\n",
    "\n",
    "# Number of tokens to generate\n",
    "num_tokens_to_generate = 300\n",
    "\n",
    "# Convert to a tensor and add batch dimension (unsqueeze(0) adds a batch dimension)\n",
    "input_tensor = torch.tensor(input_ids, dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "# Generate tokens\n",
    "generated_tokens = []\n",
    "with torch.no_grad():\n",
    "    for _ in range(num_tokens_to_generate):\n",
    "        # Get the model's prediction for the next token\n",
    "        outputs = model(input_tensor)\n",
    "        \n",
    "        # Only get the logits of the last token in the sequence\n",
    "        next_token_logits = outputs[:, -1, :]\n",
    "        \n",
    "        # Sample the next token from the probability distribution (you can also use argmax)\n",
    "        next_token = torch.multinomial(F.softmax(next_token_logits, dim=-1), num_samples=1)\n",
    "        \n",
    "        # Append the predicted token to the list of generated tokens\n",
    "        generated_tokens.append(next_token.item())\n",
    "        \n",
    "        # Append the new token to the input sequence for the next prediction\n",
    "        input_tensor = torch.cat((input_tensor, next_token.unsqueeze(0)[0]), dim=1)\n",
    "\n",
    "# Decode the generated tokens back to text\n",
    "generated_text = model.encoding.decode(generated_tokens)\n",
    "\n",
    "print(generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
