{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 8, 32])\n"
     ]
    }
   ],
   "source": [
    "class BobNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoding = tiktoken.get_encoding(\"r50k_base\")\n",
    "        self.emb_size = self.encoding.n_vocab\n",
    "        self.emb_channels = 32\n",
    "        self.emb = nn.Embedding(self.emb_size, self.emb_channels)\n",
    "        self.qW = torch.randn(self.emb_channels, self.emb_channels)\n",
    "        self.kW = torch.randn(self.emb_channels, self.emb_channels)\n",
    "        self.vW = torch.randn(self.emb_channels, self.emb_channels)\n",
    "        self.gamma = torch.randn(self.emb_channels)\n",
    "        self.beta = torch.randn(self.emb_channels)\n",
    "        self.num_heads = 8\n",
    "        self.head_dim = self.emb_channels // self.num_heads\n",
    "        assert self.head_dim * self.num_heads == self.emb_channels, \"emb_channels must be divisible by num_heads\"\n",
    "\n",
    "    def positional_encoding(self, x):\n",
    "        _, seq_length, d = x.shape\n",
    "        encoding = x.clone()\n",
    "        pos = torch.arange(seq_length).unsqueeze(1)\n",
    "        i = torch.arange(d).unsqueeze(0)\n",
    "        factor = 10000 ** (2 * i / d)\n",
    "        position_tensor = pos / factor\n",
    "        for i in i[0]:\n",
    "            encoding += torch.sin(position_tensor) if i % 2 == 0 else torch.cos(position_tensor)\n",
    "        return encoding\n",
    "    \n",
    "    def split_heads(self, x, batch_size):\n",
    "        return x.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "    def self_attention(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        # dim: (batch_size, num_heads, seq_length, head_dim)\n",
    "        q = self.split_heads(x @ self.qW, batch_size)\n",
    "        k = self.split_heads(x @ self.kW, batch_size)\n",
    "        v = self.split_heads(x @ self.vW, batch_size)\n",
    "        qK = (q @ k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "        qK = self.mask(qK)\n",
    "        attention_weights = F.softmax(qK, dim=-1)\n",
    "        output = torch.matmul(attention_weights, v)\n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.emb_channels)\n",
    "        output += x\n",
    "        output = self.layer_normalization(output)\n",
    "        return output\n",
    "        \n",
    "    def mask(self, x):\n",
    "        seq_length = x.shape[2]\n",
    "        mask = torch.tril(torch.ones((seq_length, seq_length), device=x.device))\n",
    "        mask = mask.unsqueeze(0).unsqueeze(1)\n",
    "        mask = mask.repeat(x.shape[0], self.num_heads, 1, 1)\n",
    "        return x.masked_fill(mask == 0, float('-inf'))\n",
    "    \n",
    "    def layer_normalization(self, x):\n",
    "        mean = torch.mean(x, dim=-1, keepdim=True)\n",
    "        var = torch.var(x, dim=-1, keepdim=True)\n",
    "        return self.gamma * (x - mean) / torch.sqrt(var + 1e-5) + self.beta\n",
    "    \n",
    "    def feed_forward(self, x):\n",
    "        return x\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        tokenized = [self.encoding.encode(sentence) for sentence in x]\n",
    "        max_length = max(len(t) for t in tokenized)\n",
    "        padded = [t + [0] * (max_length - len(t)) for t in tokenized]\n",
    "        input_tensor = torch.tensor(padded)\n",
    "        x = self.emb(input_tensor)\n",
    "        x = self.positional_encoding(x)\n",
    "        x = self.self_attention(x)\n",
    "        # x = self.feed_forward(x)\n",
    "        \n",
    "    \n",
    "\n",
    "sup = BobNet()\n",
    "sup(['sup bro how u doin', 'brsup bro how u doinp', 'sup bro how u'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
