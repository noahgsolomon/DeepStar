{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import tiktoken\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split:  96%|█████████▌| 71173209/74004228 [16:52<00:39, 72230.10 examples/s]"
     ]
    }
   ],
   "source": [
    "from datasets import concatenate_datasets, load_dataset\n",
    "\n",
    "bookcorpus = load_dataset(\"bookcorpus\", split=\"train\")\n",
    "wiki = load_dataset(\"wikipedia\", \"20220301.en\", split=\"train\")\n",
    "wiki = wiki.remove_columns([col for col in wiki.column_names if col != \"text\"])  # only keep the 'text' column\n",
    "\n",
    "assert bookcorpus.features.type == wiki.features.type\n",
    "raw_datasets = concatenate_datasets([bookcorpus, wiki])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "from transformers import BertTokenizerFast\n",
    "\n",
    "# repositor id for saving the tokenizer\n",
    "tokenizer_id=\"bert-base-uncased-2022-habana\"\n",
    "\n",
    "# create a python generator to dynamically load the data\n",
    "def batch_iterator(batch_size=10000):\n",
    "    for i in tqdm(range(0, len(raw_datasets), batch_size)):\n",
    "        yield raw_datasets[i : i + batch_size][\"text\"]\n",
    "\n",
    "# create a tokenizer from existing one to re-use special tokens\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bert(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoding = tiktoken.get_encoding(\"r50k_base\")\n",
    "        self.emb_size = self.encoding.n_vocab\n",
    "        self.emb_channels = 128\n",
    "        self.max_token_length = 512\n",
    "        self.emb = nn.Embedding(self.emb_size, self.emb_channels)\n",
    "        self.positional_encoding = nn.Parameter(torch.randn(self.max_token_length, self.emb_channels))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train has 301,966 tokens\n",
      "val has 36,059 tokens\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "\n",
    "# download the tiny shakespeare dataset\n",
    "input_file_path = os.path.join('', 'input.txt')\n",
    "if not os.path.exists(input_file_path):\n",
    "    data_url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
    "    with open(input_file_path, 'w') as f:\n",
    "        f.write(requests.get(data_url).text)\n",
    "\n",
    "with open(input_file_path, 'r') as f:\n",
    "    data = f.read()\n",
    "n = len(data)\n",
    "train_data = data[:int(n*0.9)]\n",
    "val_data = data[int(n*0.9):]\n",
    "\n",
    "# encode with tiktoken gpt2 bpe\n",
    "enc = tiktoken.get_encoding(\"r50k_base\")\n",
    "train_ids = enc.encode_ordinary(train_data)\n",
    "val_ids = enc.encode_ordinary(val_data)\n",
    "print(f\"train has {len(train_ids):,} tokens\")\n",
    "print(f\"val has {len(val_ids):,} tokens\")\n",
    "\n",
    "# export to bin files\n",
    "train_ids = np.array(train_ids, dtype=np.int32)\n",
    "val_ids = np.array(val_ids, dtype=np.int32)\n",
    "train_ids.tofile(os.path.join('', 'train.bin'))\n",
    "val_ids.tofile(os.path.join('', 'val.bin'))\n",
    "\n",
    "# train.bin has 301,966 tokens\n",
    "# val.bin has 36,059 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/noahs/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/noahs/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DecoderTransformer(\n",
       "  (emb): Embedding(50257, 128)\n",
       "  (l1): Linear(in_features=128, out_features=250, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (l2): Linear(in_features=250, out_features=128, bias=True)\n",
       "  (ln1): LayerNorm()\n",
       "  (ln2): LayerNorm()\n",
       "  (linear): Linear(in_features=128, out_features=50257, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "from decoder import DecoderTransformer\n",
    "from token_dataset import TokenDataset\n",
    "\n",
    "# Parameters\n",
    "seq_length = 100\n",
    "batch_size = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Dataset and DataLoader\n",
    "train_dataset = TokenDataset('train.bin', seq_length)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Model, Loss Function, Optimizer\n",
    "model = DecoderTransformer()\n",
    "crossentropy = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 8.456459999084473, count: 14:   0%|          | 0/1 [00:07<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Backward and optimize\u001b[39;00m\n\u001b[1;32m     14\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 15\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     17\u001b[0m count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import trange\n",
    "num_epochs = 5  # Number of epochs\n",
    "\n",
    "for epoch in (t:=trange(1)):\n",
    "    count = 0\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        outputs = outputs.view(-1, outputs.size(-1))\n",
    "        loss = crossentropy(outputs, targets.view(-1))\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        count += 1\n",
    "        t.set_description(f'loss: {loss.item()}, count: {count}')\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And for butUS thee,First hereCOR in3\n",
      ", truthOL talkTell wild aUN\n",
      " dishon\n",
      " dayW I Dor thatrieve up nobleUS ofORK manWhyHow thyfather\n",
      " Experts ac:Y talk,Yet H\n",
      " name? singular slave one noble thisgo me lie him:\n",
      ", is requestodes ThomasAulet long ages; stand?\n",
      " in lord I heHAM\n",
      " late! ab kneeUS you as you\n",
      "I doTh it her now. lie hereRAY\n",
      ".. fri that Tokens Sir partWAR it you shallHoldforce will\n",
      "\n",
      " majestyW is\n",
      " as. am for\n",
      ",ERrimge men\n",
      " bout another forslaveEW be th. come I. more: much Rain,One amKING '\n",
      ": shortrown,You\n",
      " look myix alas!\n",
      " good them not might, their been of most\n",
      " way kingAs, Flu. youngerTeX\n",
      "IA's be toRInt\n",
      "\n",
      "\n",
      " my but playersO markComeLAND:Which in. me boastENable inBKING my\n",
      "\n",
      " anJ\n",
      " to you straight's deatharest it the with heThe, on\n",
      " was,\n",
      " cannot\n",
      " London officeIO most hand his maid so our A\n",
      " confSLIX than notcats chastIlicts for\n",
      " betweenous.A;My are nurse\n",
      "\n",
      " thyheadUD. such,AR:\n",
      " defend t qu my hence ages-\n",
      " that\n",
      " dear byOW mischiefMER lo.,,VOL the comfort mighty\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "input_text = \" \"\n",
    "\n",
    "input_ids = model.encoding.encode(input_text)\n",
    "\n",
    "# Number of tokens to generate\n",
    "num_tokens_to_generate = 300\n",
    "\n",
    "# Convert to a tensor and add batch dimension (unsqueeze(0) adds a batch dimension)\n",
    "input_tensor = torch.tensor(input_ids, dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "# Generate tokens\n",
    "generated_tokens = []\n",
    "with torch.no_grad():\n",
    "    for _ in range(num_tokens_to_generate):\n",
    "        # Get the model's prediction for the next token\n",
    "        outputs = model(input_tensor)\n",
    "        \n",
    "        # Only get the logits of the last token in the sequence\n",
    "        next_token_logits = outputs[:, -1, :]\n",
    "        \n",
    "        # Sample the next token from the probability distribution (you can also use argmax)\n",
    "        next_token = torch.multinomial(F.softmax(next_token_logits, dim=-1), num_samples=1)\n",
    "        \n",
    "        # Append the predicted token to the list of generated tokens\n",
    "        generated_tokens.append(next_token.item())\n",
    "        \n",
    "        # Append the new token to the input sequence for the next prediction\n",
    "        input_tensor = torch.cat((input_tensor, next_token.unsqueeze(0)[0]), dim=1)\n",
    "\n",
    "# Decode the generated tokens back to text\n",
    "generated_text = model.encoding.decode(generated_tokens)\n",
    "\n",
    "print(generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
