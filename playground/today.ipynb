{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = open('../names.txt', 'r').read().splitlines()\n",
    "\n",
    "chars = sorted(set('.'.join(names)))\n",
    "\n",
    "itos = {i:c for i, c in enumerate(chars)}\n",
    "stoi = {c:i for i, c in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_len = 6\n",
    "feature_count = 25\n",
    "w_size = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(names):\n",
    "    X, Y = [], []\n",
    "\n",
    "    for name in names:\n",
    "        name += '.'\n",
    "        context = [0] * context_len\n",
    "        for ch in name:\n",
    "            X.append(context)\n",
    "            Y.append(stoi[ch])\n",
    "            context = context[1:] + [stoi[ch]]\n",
    "\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(len(names) * 0.8)\n",
    "\n",
    "random.shuffle(names)\n",
    "\n",
    "Xtrain, Ytrain = build_dataset(names[:n])\n",
    "Xval, Yval = build_dataset(names[n:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = torch.randn(27, feature_count) * 0.01\n",
    "W1 = torch.randn(feature_count*context_len, w_size) * 0.01\n",
    "b1 = torch.randn(w_size) * 0.0\n",
    "W2 = torch.randn(w_size, 27) * 0.01\n",
    "b2 = torch.randn(27) * 0.0\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 1.824375867843628: 100%|██████████| 10000/10000 [00:11<00:00, 895.77it/s]\n"
     ]
    }
   ],
   "source": [
    "optim = torch.optim.Adam(parameters)\n",
    "for _ in (t:=trange(10000)):\n",
    "    ix = torch.randint(0, Xtrain.shape[0], (50,))\n",
    "\n",
    "    emb = C[Xtrain[ix]].flatten(1)\n",
    "    hpreact = emb @ W1 + b1\n",
    "    h = torch.tanh(hpreact)\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, Ytrain[ix])\n",
    "    #cross entropy is taking the logits, exponentiating them, normalizing them, taking which value our output should be, and whatever this is taking the negative log of it such that low probabilities for the desired output result in a high loss, and outputs close to one of this probability for the expected are close to 0 loss because we are effectively waving our confidence in the correct option.\n",
    "\n",
    "    optim.zero_grad()\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    optim.step()\n",
    "\n",
    "    t.set_description(f'loss: {loss.data}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.8970)\n"
     ]
    }
   ],
   "source": [
    "emb = C[Xtrain].flatten(1)\n",
    "hpreact = emb @ W1 + b1\n",
    "h = torch.tanh(hpreact)\n",
    "logits = h @ W2 + b2\n",
    "loss = F.cross_entropy(logits, Ytrain)\n",
    "\n",
    "print(loss.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7m/w8k84tcn19d4k10r234_g4lr0000gn/T/ipykernel_98247/558715635.py:10: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = F.softmax(logits)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['sadhya.',\n",
       " 'oluwatomi.',\n",
       " 'emrest.',\n",
       " 'sephi.',\n",
       " 'karie.',\n",
       " 'draeth.',\n",
       " 'lillyse.',\n",
       " 'ezelle.',\n",
       " 'jalei.',\n",
       " 'kenli.']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = []\n",
    "for _ in range(10):\n",
    "    name = ''\n",
    "    context = [0] * context_len\n",
    "    while True:\n",
    "        emb = C[torch.tensor([context])].flatten(1)\n",
    "        hpreact = emb @ W1 + b1\n",
    "        h = torch.tanh(hpreact)\n",
    "        logits = h @ W2 + b2\n",
    "        probs = F.softmax(logits)\n",
    "\n",
    "        ix = torch.multinomial(probs, num_samples=1)[0].item()\n",
    "\n",
    "        name += itos[ix]\n",
    "        context = context[1:] + [ix]\n",
    "\n",
    "        if ix == 0:\n",
    "            res.append(name)\n",
    "            break\n",
    "    \n",
    "res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
