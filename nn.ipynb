{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import trange\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "    def __init__(self, data=1, children=[], _backward=lambda: None):\n",
    "        self.data = data\n",
    "        self.children = children\n",
    "        self._backward = _backward\n",
    "        self.grad = 0\n",
    "    \n",
    "    def log(self):\n",
    "        def _backward():\n",
    "            self.grad += (1 / self.data) * out.grad\n",
    "        out = Value(math.log(self.data), children=[self], _backward=_backward)\n",
    "        return out\n",
    "\n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data + other.data, children=(self, other))\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += 1 * out.grad\n",
    "            other.grad += 1 * out.grad\n",
    "        out._backward = _backward\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def relu(self):\n",
    "        out = Value(0.1*self.data if self.data < 0 else self.data, children=[self])\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (1 if out.data > 0 else 0.1) * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def tanh(self):\n",
    "        def _backward():\n",
    "            self.grad += (1 - pow(out.data, 2)) * out.grad\n",
    "        out = Value(math.tanh(self.data), children=[self], _backward=_backward)\n",
    "        return out\n",
    "    \n",
    "    def sigmoid(self):\n",
    "        def _backward():\n",
    "            self.grad += (out.data * (1 - out.data)) * out.grad\n",
    "        out = Value(1/(1+math.pow(math.e, -self.data)), children=[self], _backward=_backward)\n",
    "        return out\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data * other.data, children=(self, other))\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += other.data * out.grad\n",
    "            other.grad += self.data * out.grad\n",
    "\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def __pow__(self, other):\n",
    "        if isinstance(other, (float, int)):\n",
    "            def _backward():\n",
    "                self.grad += other * (self.data ** (other - 1)) * out.grad\n",
    "            out = Value(self.data ** other, children=[self], _backward=_backward)\n",
    "        \n",
    "        elif isinstance(other, Value):\n",
    "            def _backward():\n",
    "                self.grad += other.data * (self.data ** (other.data - 1)) * out.grad\n",
    "                other.grad += math.log(self.data) * (self.data ** other.data) * out.grad\n",
    "            out = Value(self.data ** other.data, children=[self, other], _backward=_backward)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'data: {self.data}'\n",
    "    \n",
    "    def backward(self):\n",
    "        topo = []\n",
    "        visited = set()\n",
    "\n",
    "        def visit(node):\n",
    "            if node not in visited:\n",
    "                visited.add(node)\n",
    "                for children in node.children:\n",
    "                    visit(children)\n",
    "                topo.append(node)\n",
    "\n",
    "        self.grad = 1\n",
    "        visit(self)\n",
    "        for item in reversed(topo):\n",
    "            item._backward()\n",
    "\n",
    "    def __sub__(self, other):\n",
    "            return self + (-other)\n",
    "    \n",
    "    def __rpow__(self, other):\n",
    "        if isinstance(other, (float, int)):\n",
    "            def _backward():\n",
    "                self.grad += other ** self.data * math.log(other) * out.grad\n",
    "            out = Value(other ** self.data, children=[self], _backward=_backward)\n",
    "            return out\n",
    "\n",
    "    def __rsub__(self, other):\n",
    "        return (self*-1) + other\n",
    "\n",
    "    def __truediv__(self, other):\n",
    "        if isinstance(other, Value):\n",
    "            def _backward():\n",
    "                self.grad += 1 / other.data * out.grad\n",
    "                other.grad -= self.data / (other.data ** 2) * out.grad\n",
    "            out = Value(self.data / other.data, children=[self, other], _backward=_backward)\n",
    "            return out\n",
    "        elif isinstance(other, (float, int)):\n",
    "            def _backward():\n",
    "                self.grad += 1 / other * out.grad\n",
    "            out = Value(self.data / other, children=[self], _backward=_backward)\n",
    "            return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class Module:\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for p in self.parameters():\n",
    "            p.grad = 0\n",
    "    \n",
    "    def parameters(self):\n",
    "        return np.array([])\n",
    "\n",
    "class Neuron(Module):\n",
    "\n",
    "    def __init__(self, nin, nonlinear=True):\n",
    "        self.w = np.array([Value(random.uniform(-1, 1)*0.01) for _ in range(nin)])\n",
    "        self.b = Value(0)\n",
    "        self.nonlinear = nonlinear\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = np.array(x)\n",
    "        out = np.dot(self.w, x) + self.b\n",
    "        return out.relu() if self.nonlinear else out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return self.w + [self.b]\n",
    "\n",
    "class Model(Module):\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "    def parameters(self):\n",
    "        return np.array([p for layer in self.layers for p in layer.parameters()])\n",
    "\n",
    "\n",
    "class Embedding(Module):\n",
    "    def __init__(self, embedding_dim, num_embeddings):\n",
    "        self.embeddings = np.array([Value(random.uniform(-1, 1)) for _ in range(embedding_dim * num_embeddings)])\n",
    "        self.embeddings = self.embeddings.reshape((num_embeddings, embedding_dim))\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "    def __call__(self, indices):\n",
    "        indices = np.array(indices, dtype=int)\n",
    "        emb = self.embeddings[indices]\n",
    "        return emb\n",
    "\n",
    "    def parameters(self):\n",
    "        return self.embeddings.flatten()\n",
    "\n",
    "class Linear(Module):\n",
    "    def __init__(self, nin, nout, **kwargs):\n",
    "        self.neurons = np.array([Neuron(nin, **kwargs) for _ in range(nout)])\n",
    "\n",
    "    def __call__(self, x):\n",
    "        out = np.array([neuron(x) for neuron in self.neurons])\n",
    "        return out.flatten() if out.size == 1 else out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return np.array([p for neuron in self.neurons for p in neuron.parameters()])\n",
    "    \n",
    "\n",
    "# class MLP(Module):\n",
    "#     def __init__(self, nin, nouts):\n",
    "#         layers = [nin] + nouts\n",
    "#         self.layers = [Linear(layers[i], layers[i+1], nonlinear=(i!=len(nouts)-1)) for i in range(len(layers)-1)]\n",
    "    \n",
    "#     def __call__(self, x):\n",
    "#         for layer in self.layers:\n",
    "#             x = layer(x)\n",
    "#         return x\n",
    "\n",
    "#     def parameters(self):\n",
    "#         return [p for layer in self.layers for neuron in layer.neurons for p in neuron.parameters()]\n",
    "    \n",
    "#     def equation(self):\n",
    "#         val = ''\n",
    "#         layerEquations = []\n",
    "\n",
    "#         for layer in self.layers:\n",
    "\n",
    "#             neuronEquations = []\n",
    "#             for neuron in layer.neurons:\n",
    "\n",
    "#                 equation = ''\n",
    "#                 if len(layerEquations) == 0:\n",
    "\n",
    "#                     for w in neuron.w:\n",
    "#                         equation += f'{w.data:.2f}X + '\n",
    "\n",
    "#                 else:\n",
    "#                     for w in neuron.w:\n",
    "#                         equation += f'{w.data:.2f}({layerEquations[-1]}) + '\n",
    "\n",
    "#                 if neuron.nonlinear:\n",
    "#                     neuronEquations.append(f'tanh({equation}) + {neuron.b.data} + ')\n",
    "#                 else:\n",
    "#                     neuronEquations.append(f'{equation}{neuron.b.data} + ')\n",
    "\n",
    "#             fullLayerEquation = ''\n",
    "\n",
    "#             for neuronEquation in neuronEquations:\n",
    "#                 fullLayerEquation += neuronEquation\n",
    "\n",
    "#             layerEquations.append(fullLayerEquation)\n",
    "\n",
    "#         for layerEquation in layerEquations:\n",
    "#             val += layerEquation\n",
    "        \n",
    "#         return val.replace(' + )', '').replace(' + ,', '')[:-3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_len = 5\n",
    "feature_count = 25\n",
    "\n",
    "words = open('words.txt', 'r').read().splitlines()\n",
    "\n",
    "random.shuffle(words)\n",
    "\n",
    "chars = sorted(set('.'.join(words)))\n",
    "\n",
    "itos = {i: c for i, c in enumerate(chars)}\n",
    "stoi = {c: i for i, c in enumerate(chars)}\n",
    "\n",
    "def build_dataset(words):\n",
    "    X, Y = [], []\n",
    "\n",
    "    for word in words:\n",
    "        context = [0] * context_len\n",
    "        word += '.'\n",
    "        for ch in word:\n",
    "            X.append(context)\n",
    "            Y.append(stoi[ch])\n",
    "            context = context[1:] + [stoi[ch]]\n",
    "\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = build_dataset(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding(27, 25)\n"
     ]
    }
   ],
   "source": [
    "class Bob(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Bob, self).__init__()\n",
    "        self.C = nn.Embedding(27, feature_count)\n",
    "        print(self.C)\n",
    "        self.l1 = nn.Linear(feature_count*context_len, 200)\n",
    "        self.l2 = nn.Linear(200, 27)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.C(x).flatten(1)\n",
    "        x = F.relu(self.l1(x))\n",
    "        x = self.l2(x)\n",
    "        return x\n",
    "\n",
    "model = Bob()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 2557.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 5])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for _ in (t:=trange(1)):\n",
    "    ix = torch.randint(0, X.shape[0], (32,))\n",
    "    print(X[ix].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 5, 20)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(25940,)"
      ]
     },
     "execution_count": 406,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "C = Embedding(20, 27)\n",
    "emb = C(X[ix])\n",
    "print(emb.shape)\n",
    "l1 = Linear(20*5, 200)\n",
    "l2 = Linear(200, 27)\n",
    "\n",
    "model = Model([C, l1, l2])\n",
    "\n",
    "model.parameters().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  0,  3,  9, 20])"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[ix][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 1.8727288246154785: 100%|██████████| 10000/10000 [00:09<00:00, 1092.66it/s]\n"
     ]
    }
   ],
   "source": [
    "for _ in (t:=trange(10000)):\n",
    "    ix = torch.randint(0, X.shape[0], (32,))\n",
    "    logits = model(X[ix])\n",
    "    loss = F.cross_entropy(logits, Y[ix])\n",
    "\n",
    "    optim.zero_grad()\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    optim.step()\n",
    "    t.set_description(f'loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tomadoy.\n",
      "opnicting.\n",
      "intransmet.\n",
      "quarter.\n",
      "backn.\n",
      "maniah.\n",
      "intenvitica.\n",
      "simouncy.\n",
      "inamy.\n",
      "nutout.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7m/w8k84tcn19d4k10r234_g4lr0000gn/T/ipykernel_34045/2618988002.py:6: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = F.softmax(out)\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    word = ''\n",
    "    context = [0] * context_len\n",
    "    while True:\n",
    "        out = model(torch.tensor([context]))\n",
    "        probs = F.softmax(out)\n",
    "        ix = torch.multinomial(probs, num_samples=1)[0].item()\n",
    "        word += itos[ix]\n",
    "        context = context[1:] + [ix]\n",
    "        if ix == 0:\n",
    "            print(word)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = torch.randn(feature_count, 27)\n",
    "W1 = torch.randn(feature_count*context_len, 200)\n",
    "b1 = torch.randn(200)\n",
    "W2 = torch.randn(200, 27)\n",
    "b2 = torch.randn(27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_reg = 0.01\n",
    "batch_size = 16\n",
    "\n",
    "for _ in (t:=trange(5)):\n",
    "    batch_loss = 0  # Initialize loss for the batch\n",
    "\n",
    "    # Accumulate gradients over the batch\n",
    "    for _ in range(batch_size):\n",
    "        ix = int(numpy.random.randint(0, X.shape[0], (1,)))\n",
    "\n",
    "        input = [[0.] * 27 for _ in range(X.shape[1])]\n",
    "\n",
    "        for i in range(X.shape[1]):\n",
    "            input[i][int(X[ix][i])] = 1.0\n",
    "\n",
    "        out = model(list(np.array(input).flatten()))\n",
    "\n",
    "        expected = Y[ix]\n",
    "\n",
    "        maxVal = max([num.data for num in out])\n",
    "\n",
    "        exp = [(2**(num-maxVal)) for num in out]\n",
    "\n",
    "        count = sum([num.data for num in exp])\n",
    "\n",
    "        prob = [val/count for val in exp]\n",
    "\n",
    "        loss = prob[int(Y[ix])].log()*-1\n",
    "        batch_loss += loss.data  # Accumulate loss\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "    # Update parameters after processing the batch\n",
    "    for p in model.parameters():\n",
    "        p.data -= p.grad * 0.1 / batch_size + lambda_reg * p.data**2 \n",
    "        p.grad = 0\n",
    "    \n",
    "    t.set_description(f'Average batch loss: {batch_loss / batch_size:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix = int(numpy.random.randint(0, X.shape[0], (1,)))\n",
    "\n",
    "input = [[0.] * 27 for _ in range(X.shape[1])]\n",
    "\n",
    "for i in range(X.shape[1]):\n",
    "    input[i][int(X[ix][i])] = 1.0\n",
    "\n",
    "out = model(list(np.array(input).flatten()))\n",
    "expected = Y[ix]\n",
    "\n",
    "maxVal = max([num.data for num in out])\n",
    "\n",
    "exp = [(2**(num-maxVal)) for num in out]\n",
    "\n",
    "\n",
    "count = sum([num.data for num in exp])\n",
    "\n",
    "prob = [val/count for val in exp]\n",
    "\n",
    "loss = prob[int(Y[ix])].log()*-1\n",
    "\n",
    "predict = (0, 0)\n",
    "for i, p in enumerate(prob):\n",
    "    if p.data > predict[1]:\n",
    "        predict = (i, p.data)\n",
    "\n",
    "print(f'input: {list(np.array(input).flatten())} predicted: {predict}, actual: {expected}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(10, [1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minRange = -100\n",
    "maxRange = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.linspace(minRange, maxRange, 100)\n",
    "Y = np.linspace(minRange, maxRange, 100)\n",
    "A = np.linspace(minRange, maxRange, 100)\n",
    "B = np.linspace(minRange, maxRange, 100)\n",
    "C = np.linspace(minRange, maxRange, 100)\n",
    "D = np.linspace(minRange, maxRange, 100)\n",
    "E = np.linspace(minRange, maxRange, 100)\n",
    "F = np.linspace(minRange, maxRange, 100)\n",
    "G = np.linspace(minRange, maxRange, 100)\n",
    "H = np.linspace(minRange, maxRange, 100)\n",
    "\n",
    "random.shuffle(X)\n",
    "random.shuffle(Y)\n",
    "random.shuffle(A)\n",
    "random.shuffle(B)\n",
    "random.shuffle(C)\n",
    "random.shuffle(D)\n",
    "random.shuffle(E)\n",
    "random.shuffle(F)\n",
    "random.shuffle(G)\n",
    "random.shuffle(H)\n",
    "\n",
    "\n",
    "def Z(x, y, a, b, c, d, e, f, g, h):\n",
    "    return x + 33*y + 2*a + 3*b + 4*c + 5*d + 6*e + 33*f - 10*g + 0.5*h + 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for _ in (t:=trange(100)):\n",
    "    loss_avg = 0\n",
    "    random.shuffle(X)\n",
    "    random.shuffle(Y)\n",
    "    random.shuffle(A)\n",
    "    random.shuffle(B)\n",
    "    random.shuffle(C)\n",
    "    random.shuffle(D)\n",
    "    random.shuffle(E)\n",
    "    random.shuffle(F)\n",
    "    random.shuffle(G)\n",
    "    random.shuffle(H)\n",
    "    for x in X[:2]:\n",
    "        for y in Y[:2]:\n",
    "            for a in A[:2]:\n",
    "                for b in B[:2]:\n",
    "                    for c in C[:2]:\n",
    "                        for d in D[:2]:\n",
    "                            for e in E[:2]:\n",
    "                                for f in F[:2]:\n",
    "                                    for g in G[:2]:\n",
    "                                        for h in H[:2]:\n",
    "                                            out = model([x, y, a, b, c, d, e, f, g, h])\n",
    "\n",
    "                                            loss = (out-Z(x, y, a, b, c, d, e, f, g, h))**2\n",
    "\n",
    "                                            loss_avg += loss.data\n",
    "\n",
    "                                            loss.backward()\n",
    "\n",
    "                                            for p in model.parameters():\n",
    "                                                p.data -= 0.00001 * p.grad\n",
    "                                                p.grad = 0\n",
    "\n",
    "    t.set_description(f'{loss_avg / (X.shape[0]*Y.shape[0])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix = np.random.randint(-100, 100, (1,))\n",
    "iy = np.random.randint(-100, 100, (1,))\n",
    "ia = np.random.randint(-100, 100, (1,))\n",
    "ib = np.random.randint(-100, 100, (1,))\n",
    "ic = np.random.randint(-100, 100, (1,))\n",
    "id = np.random.randint(-100, 100, (1,))\n",
    "ie = np.random.randint(-100, 100, (1,))\n",
    "iff = np.random.randint(-100, 100, (1,))\n",
    "ig = np.random.randint(-100, 100, (1,))\n",
    "ih = np.random.randint(-100, 100, (1,))\n",
    "\n",
    "out = model([ix, iy, ia, ib, ic, id, ie, iff, ig, ih])\n",
    "\n",
    "print(f'diff: {out-Z(ix, iy, ia, ib, ic, id, ie, iff, ig, ih)}')\n",
    "\n",
    "# loss = (out-z)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.linspace(-5, 5, 100)\n",
    "Y = np.linspace(-5, 5, 100)\n",
    "\n",
    "X, Y = np.meshgrid(X, Y)\n",
    "\n",
    "Z_predicted = np.array([model([x, y]).data for x, y in zip(X.flatten(), Y.flatten())])\n",
    "\n",
    "# Reshape Z_predicted to match the shape of X and Y\n",
    "Z_predicted = Z_predicted.reshape(X.shape)\n",
    "\n",
    "print(Z_predicted.shape)\n",
    "\n",
    "Z = X + 33*Y + 8\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot_surface(X, Y, Z_predicted, color='red', label='Predicted Z')\n",
    "ax.plot_surface(X, Y, Z, alpha=0.5, color='blue', label='Actual Z')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(1, [1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.uniform(-5, 5, 1000)\n",
    "Y = X**3 + X**2 + X + 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in (t:=trange(100)):\n",
    "    loss_avg = 0\n",
    "    count = 0\n",
    "    for x, y in zip(X, Y):  \n",
    "        out = model([x**3 + x**2 + x])\n",
    "\n",
    "        loss = (out - y)**2\n",
    "\n",
    "        loss_avg += loss.data\n",
    "        count += 1\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        for p in model.parameters():\n",
    "            p.data -= 0.00001 * p.grad\n",
    "            p.grad = 0\n",
    "\n",
    "    t.set_description(f'loss: {loss_avg / max(count, 1)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_predicted = np.array([model([x**3 + x**2 + x]).data for x in X])\n",
    "\n",
    "plt.scatter(X, Y, label='Actual')\n",
    "plt.scatter(X, Y_predicted, label='Predicted')\n",
    "\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bottleneck effect\n",
    "\n",
    "X = np.random.uniform(-1, 1, 100)\n",
    "Y = np.random.uniform(-1, 1, 100)\n",
    "A = np.random.uniform(-1, 1, 100)\n",
    "B = np.random.uniform(-1, 1, 100)\n",
    "C = np.random.uniform(-1, 1, 100)\n",
    "D = np.random.uniform(-1, 1, 100)\n",
    "E = np.random.uniform(-1, 1, 100)\n",
    "F = np.random.uniform(-1, 1, 100)\n",
    "G = np.random.uniform(-1, 1, 100)\n",
    "H = np.random.uniform(-1, 1, 100)\n",
    "\n",
    "def Z(x, y, a, b, c, d, e, f, g, h):\n",
    "    return [[x+y+a+b+c+d+e+f+g+h],[2*x+2*y+2*a+2*b+3*c+8*d+9*e+f+g+h],[8*x+8*y+8*a+8*b+3*c+8*d+9*e+f+g+h],[11*x+11*y+11*a+11*b+3*c+8*d+9*e+f+g+h], [x+y+a+b+c+d+e+f+g+h],[2*x+2*y+2*a+2*b+3*c+8*d+9*e+f+g+h],[8*x+8*y+8*a+8*b+3*c+8*d+9*e+f+g+h],[11*x+11*y+11*a+11*b+3*c+8*d+9*e+f+g+h], [x+y+a+b+c+d+e+f+g+h],[2*x+2*y+2*a+2*b+3*c+8*d+9*e+f+g+h],[8*x+8*y+8*a+8*b+3*c+8*d+9*e+f+g+h],[11*x+11*y+11*a+11*b+3*c+8*d+9*e+f+g+h]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(10, [12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "avg_loss = 0\n",
    "count = 0\n",
    "for _ in (t:=trange(10000)):\n",
    "    X = np.random.uniform(-1, 1, 100)\n",
    "    Y = np.random.uniform(-1, 1, 100)\n",
    "    A = np.random.uniform(-1, 1, 100)\n",
    "    B = np.random.uniform(-1, 1, 100)\n",
    "    C = np.random.uniform(-1, 1, 100)\n",
    "    D = np.random.uniform(-1, 1, 100)\n",
    "    E = np.random.uniform(-1, 1, 100)\n",
    "    F = np.random.uniform(-1, 1, 100)\n",
    "    G = np.random.uniform(-1, 1, 100)\n",
    "    H = np.random.uniform(-1, 1, 100)\n",
    "    i += 1\n",
    "\n",
    "    for x in X[:1]:\n",
    "        for y in Y[:1]:\n",
    "            for a in A[:1]:\n",
    "                for b in B[:1]:\n",
    "                    for c in C[:1]:\n",
    "                        for d in D[:1]:\n",
    "                            for e in E[:1]:\n",
    "                                for f in F[:1]:\n",
    "                                    for g in G[:1]:\n",
    "                                        for h in H[:1]:\n",
    "                                            out = model([x, y, a, b, c, d, e, f, g, h])\n",
    "                                            z = Z(x, y, a, b, c, d, e, f, g, h)\n",
    "                                            loss = (out[0]-z[0][0])**2 + (out[1]-z[1][0])**2 + (out[2]-z[2][0])**2 + (out[3]-z[3][0])**2 + (out[4]-z[4][0])**2 + (out[5]-z[5][0])**2 + (out[6]-z[6][0])**2 + (out[7]-z[7][0])**2 + (out[8]-z[8][0])**2 + (out[9]-z[9][0])**2 + (out[10]-z[10][0])**2 + (out[11]-z[11][0])**2\n",
    "\n",
    "                                            avg_loss += loss.data\n",
    "                                            count += 1\n",
    "\n",
    "                                            loss.backward()\n",
    "\n",
    "                                            for p in model.parameters():\n",
    "                                                p.data -= 0.0001 * p.grad\n",
    "                                                p.grad = 0\n",
    "\n",
    "print(f'loss: {avg_loss/(max(count, 1))}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottleneck_model = MLP(10, [2, 12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "avg_loss = 0\n",
    "count = 0\n",
    "for _ in (t:=trange(10000)):\n",
    "    X = np.random.uniform(-1, 1, 100)\n",
    "    Y = np.random.uniform(-1, 1, 100)\n",
    "    A = np.random.uniform(-1, 1, 100)\n",
    "    B = np.random.uniform(-1, 1, 100)\n",
    "    C = np.random.uniform(-1, 1, 100)\n",
    "    D = np.random.uniform(-1, 1, 100)\n",
    "    E = np.random.uniform(-1, 1, 100)\n",
    "    F = np.random.uniform(-1, 1, 100)\n",
    "    G = np.random.uniform(-1, 1, 100)\n",
    "    H = np.random.uniform(-1, 1, 100)\n",
    "    i += 1\n",
    "\n",
    "    for x in X[:1]:\n",
    "        for y in Y[:1]:\n",
    "            for a in A[:1]:\n",
    "                for b in B[:1]:\n",
    "                    for c in C[:1]:\n",
    "                        for d in D[:1]:\n",
    "                            for e in E[:1]:\n",
    "                                for f in F[:1]:\n",
    "                                    for g in G[:1]:\n",
    "                                        for h in H[:1]:\n",
    "                                            out = bottleneck_model([x, y, a, b, c, d, e, f, g, h])\n",
    "                                            z = Z(x, y, a, b, c, d, e, f, g, h)\n",
    "                                            loss = (out[0]-z[0][0])**2 + (out[1]-z[1][0])**2 + (out[2]-z[2][0])**2 + (out[3]-z[3][0])**2 + (out[4]-z[4][0])**2 + (out[5]-z[5][0])**2 + (out[6]-z[6][0])**2 + (out[7]-z[7][0])**2 + (out[8]-z[8][0])**2 + (out[9]-z[9][0])**2 + (out[10]-z[10][0])**2 + (out[11]-z[11][0])**2\n",
    "\n",
    "                                            avg_loss += loss.data\n",
    "                                            count += 1\n",
    "\n",
    "                                            loss.backward()\n",
    "\n",
    "                                            for p in bottleneck_model.parameters():\n",
    "                                                p.data -= 0.0001 * p.grad\n",
    "                                                p.grad = 0\n",
    "\n",
    "print(f'loss: {avg_loss/(max(count, 1))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(2, [3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_loss = 0\n",
    "count = 10_000\n",
    "for _ in (t:=trange(count)):\n",
    "    ix = np.random.randint(0, X.shape[0], (1,)).item()\n",
    "    iy = np.random.randint(0, Y.shape[0], (1,)).item()\n",
    "\n",
    "    k = Z(X[ix], Y[iy])\n",
    "    \n",
    "    out = model([X[ix], Y[iy]])\n",
    "\n",
    "    exp = [(math.e**(v)) for v in out]\n",
    "\n",
    "    countV = sum([v.data for v in exp])\n",
    "\n",
    "    prob = [val/countV for val in exp]\n",
    "\n",
    "    loss = prob[k].log()\n",
    "\n",
    "    avg_loss += loss.data\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    for p in model.parameters():\n",
    "        p.data += 0.0000001 * p.grad\n",
    "        p.grad = 0\n",
    "\n",
    "    t.set_description(f'prob: {prob[k]}')\n",
    "print(f'avg loss: {-avg_loss/count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "trials = 10_000\n",
    "for _ in range(trials):\n",
    "    ix = np.random.randint(0, X.shape[0], (1,)).item()\n",
    "    iy = np.random.randint(0, Y.shape[0], (1,)).item()\n",
    "\n",
    "    k = Z(X[ix], Y[iy], A[ia])\n",
    "        \n",
    "    out = model([X[ix], Y[iy], A[ia]])\n",
    "\n",
    "    exp = [(math.e**(v)) for v in out]\n",
    "\n",
    "    countV = sum([v.data for v in exp])\n",
    "\n",
    "    prob = [val/countV for val in exp]\n",
    "\n",
    "    if prob[k].data == max([p.data for p in prob]):\n",
    "        correct += 1\n",
    "    \n",
    "print(f'accuracy: {correct/trials*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = {}\n",
    "for _ in range(100_000):\n",
    "    ix = np.random.randint(0, X.shape[0], (1,)).item()\n",
    "    iy = np.random.randint(0, Y.shape[0], (1,)).item()\n",
    "\n",
    "    k = Z(X[ix], Y[iy])\n",
    "    out[k] = out.get(k, 0) + 1\n",
    "\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi class \n",
    "X = np.random.uniform(-10, 10, 1000)\n",
    "Y = np.random.uniform(-10, 10, 1000)\n",
    "A = np.random.uniform(-10, 10, 1000)\n",
    "B = np.random.uniform(-10, 10, 1000)\n",
    "\n",
    "def Z(x, y, a, b):\n",
    "    value = np.sin(x) * np.cos(y) + np.tanh(a * b)\n",
    "\n",
    "    if value < -0.5:\n",
    "        return 0\n",
    "    elif -0.5 <= value < 0.5:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = torch.randn(4, 20) * 0.01\n",
    "b1 = torch.randn(20) * 0.0\n",
    "W2 = torch.randn(20, 3) * 0.01\n",
    "b2 = torch.randn(3) * 0.0\n",
    "\n",
    "parameters = [W1, b1, W2, b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.Adam(parameters)\n",
    "for _ in (t:=trange(10_000)):\n",
    "    ix = np.random.randint(0, X.shape[0], (1,)).item()\n",
    "    iy = np.random.randint(0, Y.shape[0], (1,)).item()\n",
    "    ia = np.random.randint(0, A.shape[0], (1,)).item()\n",
    "    ib = np.random.randint(0, B.shape[0], (1,)).item()\n",
    "\n",
    "    k = Z(X[ix], Y[iy], A[ia], B[ib])\n",
    "    input = torch.tensor([X[ix], Y[iy], A[ia], B[ib]]).float()\n",
    "    hpreact = input @ W1 + b1\n",
    "    h = torch.tanh(hpreact)\n",
    "    logits = h @ W2 + b2\n",
    "\n",
    "    optim.zero_grad()\n",
    "    \n",
    "    loss = F.cross_entropy(logits, torch.tensor(k))\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    optim.step()\n",
    "\n",
    "    t.set_description(f'loss: {loss.data}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "trials = 10_000\n",
    "for _ in range(trials):\n",
    "    ix = np.random.randint(0, X.shape[0], (1,)).item()\n",
    "    iy = np.random.randint(0, Y.shape[0], (1,)).item()\n",
    "    ia = np.random.randint(0, A.shape[0], (1,)).item()\n",
    "    ib = np.random.randint(0, B.shape[0], (1,)).item()\n",
    "\n",
    "    k = Z(X[ix], Y[iy], A[ia], B[ib])\n",
    "    input = torch.tensor([X[ix], Y[iy], A[ia], B[ib]]).float()\n",
    "    hpreact = input @ W1 + b1\n",
    "    h = torch.tanh(hpreact)\n",
    "    logits = h @ W2 + b2\n",
    "\n",
    "    probs = F.softmax(logits)\n",
    "\n",
    "    out = torch.multinomial(probs, num_samples=1)[0].item()\n",
    "\n",
    "    if torch.argmax(probs) == out:\n",
    "        correct += 1\n",
    "    \n",
    "print(f'accuracy: {correct/trials*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def fetch(url):\n",
    "    import requests, gzip, os, hashlib, numpy as np\n",
    "    fp = os.path.join('tmp/' + hashlib.md5(url.encode('utf-8')).hexdigest() + '.gz')\n",
    "    if os.path.isfile(fp):\n",
    "        with open(fp, 'rb') as f:\n",
    "            dat = f.read()\n",
    "    else:\n",
    "        with open(fp, 'wb') as f:\n",
    "            dat = requests.get(url).content\n",
    "            f.write(dat)\n",
    "    return np.frombuffer(gzip.decompress(dat), dtype=np.uint8).copy()\n",
    "\n",
    "X_train = torch.tensor(fetch(\"http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\")[16:].reshape((-1, 28, 28)))\n",
    "Y_train = torch.tensor(fetch(\"http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\")[8:])\n",
    "X_test = torch.tensor(fetch(\"http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\")[16:].reshape((-1, 28, 28)))\n",
    "Y_test = torch.tensor(fetch(\"http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\")[8:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = torch.randn(28*28, 200) * 0.01\n",
    "b1 = torch.randn(200) * 0.0\n",
    "W2 = torch.randn(200, 100) * 0.01\n",
    "b2 = torch.randn(100) * 0.0\n",
    "W3 = torch.randn(100, 10) * 0.01\n",
    "b3 = torch.randn(10) * 0.0\n",
    "\n",
    "parameters = [W1, b1, W2, b2, W3, b3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.Adam(parameters)\n",
    "for _ in (t:=trange(1000)):\n",
    "    ix = torch.randint(0, X_train.shape[0], (32,))\n",
    "    l1 = torch.tanh(X_train[ix].flatten(1).float() @ W1 + b1)\n",
    "    l2 = torch.tanh(l1 @ W2 + b2)\n",
    "    logits = l2 @ W3 + b3\n",
    "\n",
    "    loss = F.cross_entropy(logits, Y_train[ix])\n",
    "\n",
    "    optim.zero_grad()\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    optim.step()\n",
    "\n",
    "    t.set_description(f'loss: {loss.data}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "trials = 100000\n",
    "for _ in range(trials):\n",
    "    ix = torch.randint(0, X_train.shape[0], (1,))\n",
    "    l1 = torch.tanh(X_train[ix].flatten(1).float() @ W1 + b1)\n",
    "    l2 = torch.tanh(l1 @ W2 + b2)\n",
    "    logits = l2 @ W3 + b3\n",
    "\n",
    "    prob = F.softmax(logits)\n",
    "\n",
    "    if torch.argmax(prob).item() == Y_train[ix].item():\n",
    "        correct += 1\n",
    "\n",
    "print(f'accuracy: {correct/trials}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
