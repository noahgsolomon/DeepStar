{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import trange\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "    def __init__(self, data=1, children=[], _backward=lambda: None, _op=''):\n",
    "        self.data = data\n",
    "        self.children = children\n",
    "        self._backward = _backward\n",
    "        self.grad = 0\n",
    "        self._op = _op\n",
    "    \n",
    "    def log(self):\n",
    "        def _backward():\n",
    "            self.grad += (1 / self.data) * out.grad\n",
    "        out = Value(math.log(self.data), children=[self], _backward=_backward, _op='log')\n",
    "        return out\n",
    "\n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data + other.data, children=(self, other), _op='+')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += 1 * out.grad\n",
    "            other.grad += 1 * out.grad\n",
    "        out._backward = _backward\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def relu(self):\n",
    "        out = Value(0.1*self.data if self.data < 0 else self.data, children=[self], _op='relu')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (1 if out.data > 0 else 0.1) * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def tanh(self):\n",
    "        def _backward():\n",
    "            self.grad += (1 - pow(out.data, 2)) * out.grad\n",
    "        out = Value(math.tanh(self.data), children=[self], _backward=_backward, _op='tanh')\n",
    "        return out\n",
    "    \n",
    "    def sigmoid(self):\n",
    "        def _backward():\n",
    "            self.grad += (out.data * (1 - out.data)) * out.grad\n",
    "        out = Value(1/(1+math.pow(math.e, -self.data)), children=[self], _backward=_backward)\n",
    "        return out\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data * other.data, children=(self, other), _op='*')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += other.data * out.grad\n",
    "            other.grad += self.data * out.grad\n",
    "\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def __pow__(self, other):\n",
    "        if isinstance(other, (float, int)):\n",
    "            def _backward():\n",
    "                self.grad += other * (self.data ** (other - 1)) * out.grad\n",
    "            out = Value(self.data ** other, children=[self], _backward=_backward, _op=f'**{other}')\n",
    "        \n",
    "        elif isinstance(other, Value):\n",
    "            def _backward():\n",
    "                self.grad += other.data * (self.data ** (other.data - 1)) * out.grad\n",
    "                other.grad += math.log(self.data) * (self.data ** other.data) * out.grad\n",
    "            out = Value(self.data ** other.data, children=[self, other], _backward=_backward, _op=f'**{other.data}')\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def exp(self):\n",
    "        def _backward():\n",
    "            self.grad += math.exp(self.data) * out.grad\n",
    "        out = Value(math.exp(self.data), children=[self], _backward=_backward, _op='exp')\n",
    "        return out\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'data: {self.data}, grad: {self.grad}, op: {self._op}'\n",
    "    \n",
    "    def backward(self):\n",
    "        topo = []\n",
    "        visited = set()\n",
    "\n",
    "        def visit(node):\n",
    "            if node not in visited:\n",
    "                visited.add(node)\n",
    "                for children in node.children:\n",
    "                    visit(children)\n",
    "                topo.append(node)\n",
    "\n",
    "        self.grad = 1\n",
    "        visit(self)\n",
    "        for item in reversed(topo):\n",
    "            item._backward()\n",
    "\n",
    "    def step(self):\n",
    "        topo = []\n",
    "        visited = set()\n",
    "\n",
    "        def visit(node):\n",
    "            if node not in visited:\n",
    "                visited.add(node)\n",
    "                for children in node.children:\n",
    "                    visit(children)\n",
    "                topo.append(node)\n",
    "\n",
    "        self.grad = 1\n",
    "        visit(self)\n",
    "        for item in reversed(topo):\n",
    "            item.data -= item.grad * 0.01\n",
    "\n",
    "    def __sub__(self, other):\n",
    "            return self + (-other)\n",
    "    \n",
    "    def __rpow__(self, other):\n",
    "        if isinstance(other, (float, int)):\n",
    "            def _backward():\n",
    "                self.grad += other ** self.data * math.log(other) * out.grad\n",
    "            out = Value(other ** self.data, children=[self], _backward=_backward, _op=f'**{other}')\n",
    "            return out\n",
    "\n",
    "    def __rsub__(self, other):\n",
    "        return (self*-1) + other\n",
    "    \n",
    "    def __radd__(self, other):\n",
    "        return self + other\n",
    "\n",
    "    def __truediv__(self, other):\n",
    "        if isinstance(other, Value):\n",
    "            def _backward():\n",
    "                self.grad += 1 / other.data * out.grad\n",
    "                other.grad -= self.data / (other.data ** 2) * out.grad\n",
    "            out = Value(self.data / other.data, children=[self, other], _backward=_backward, _op='รท')\n",
    "            return out\n",
    "        elif isinstance(other, (float, int)):\n",
    "            def _backward():\n",
    "                self.grad += 1 / other * out.grad\n",
    "            out = Value(self.data / other, children=[self], _backward=_backward, _op='รท')\n",
    "            return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class Module:\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for p in self.parameters():\n",
    "            p.grad = 0\n",
    "    \n",
    "    def parameters(self):\n",
    "        return np.array([])\n",
    "\n",
    "class Neuron(Module):\n",
    "\n",
    "    def __init__(self, nin, nonlinear=True):\n",
    "        self.w = [Value(random.uniform(-1, 1)) for _ in range(nin)]\n",
    "        self.b = Value(0)\n",
    "        self.nonlinear = nonlinear\n",
    "\n",
    "    def __call__(self, x):\n",
    "        out = sum(w * x_ for w, x_ in zip(self.w, x)) + self.b\n",
    "        return out.tanh() if self.nonlinear else out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return self.w + [self.b]\n",
    "\n",
    "class Model(Module):\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "    def parameters(self):\n",
    "        return np.array([p for layer in self.layers for p in layer.parameters()])\n",
    "\n",
    "\n",
    "class Embedding(Module):\n",
    "    def __init__(self, embedding_dim, num_embeddings):\n",
    "        self.embeddings = np.array([Value(random.uniform(-1, 1)) for _ in range(embedding_dim * num_embeddings)]).reshape((num_embeddings, embedding_dim))\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "    def __call__(self, indices):\n",
    "        return self.embeddings[np.array(indices, dtype=int)].flatten()\n",
    "\n",
    "    def parameters(self):\n",
    "        return self.embeddings.flatten()\n",
    "\n",
    "class Linear(Module):\n",
    "    def __init__(self, nin, nout, **kwargs):\n",
    "        self.neurons = [Neuron(nin, **kwargs) for _ in range(nout)] \n",
    "\n",
    "    def __call__(self, x):\n",
    "        out = [neuron(x) for neuron in self.neurons]  # This remains a list comprehension\n",
    "        return out if len(out) > 1 else out[0]\n",
    "    \n",
    "    def parameters(self):\n",
    "        return np.array([p for neuron in self.neurons for p in neuron.parameters()])\n",
    "    \n",
    "class Bob:\n",
    "    def cross_entropy(logits, actual):\n",
    "        maxVal = max([num.data for num in logits])\n",
    "\n",
    "        exp = [(2**(num-maxVal)) for num in logits]\n",
    "\n",
    "        count = sum([num.data for num in exp])\n",
    "\n",
    "        prob = [val/count for val in exp]\n",
    "\n",
    "        loss = prob[int(actual)].log()*-1\n",
    "\n",
    "        return loss\n",
    "\n",
    "# class MLP(Module):\n",
    "#     def __init__(self, nin, nouts):\n",
    "#         layers = [nin] + nouts\n",
    "#         self.layers = [Linear(layers[i], layers[i+1], nonlinear=(i!=len(nouts)-1)) for i in range(len(layers)-1)]\n",
    "    \n",
    "#     def __call__(self, x):\n",
    "#         for layer in self.layers:\n",
    "#             x = layer(x)\n",
    "#         return x\n",
    "\n",
    "#     def parameters(self):\n",
    "#         return [p for layer in self.layers for neuron in layer.neurons for p in neuron.parameters()]\n",
    "    \n",
    "#     def equation(self):\n",
    "#         val = ''\n",
    "#         layerEquations = []\n",
    "\n",
    "#         for layer in self.layers:\n",
    "\n",
    "#             neuronEquations = []\n",
    "#             for neuron in layer.neurons:\n",
    "\n",
    "#                 equation = ''\n",
    "#                 if len(layerEquations) == 0:\n",
    "\n",
    "#                     for w in neuron.w:\n",
    "#                         equation += f'{w.data:.2f}X + '\n",
    "\n",
    "#                 else:\n",
    "#                     for w in neuron.w:\n",
    "#                         equation += f'{w.data:.2f}({layerEquations[-1]}) + '\n",
    "\n",
    "#                 if neuron.nonlinear:\n",
    "#                     neuronEquations.append(f'tanh({equation}) + {neuron.b.data} + ')\n",
    "#                 else:\n",
    "#                     neuronEquations.append(f'{equation}{neuron.b.data} + ')\n",
    "\n",
    "#             fullLayerEquation = ''\n",
    "\n",
    "#             for neuronEquation in neuronEquations:\n",
    "#                 fullLayerEquation += neuronEquation\n",
    "\n",
    "#             layerEquations.append(fullLayerEquation)\n",
    "\n",
    "#         for layerEquation in layerEquations:\n",
    "#             val += layerEquation\n",
    "        \n",
    "#         return val.replace(' + )', '').replace(' + ,', '')[:-3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "def trace(root):\n",
    "  # builds a set of all nodes and edges in a graph\n",
    "  nodes, edges = set(), set()\n",
    "  def build(v):\n",
    "    if v not in nodes:\n",
    "      nodes.add(v)\n",
    "      for child in v.children:\n",
    "        edges.add((child, v))\n",
    "        build(child)\n",
    "  build(root)\n",
    "  return nodes, edges\n",
    "\n",
    "def draw_dot(root):\n",
    "  dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'}) # LR = left to right\n",
    "  \n",
    "  nodes, edges = trace(root)\n",
    "  for n in nodes:\n",
    "    uid = str(id(n))\n",
    "    # for any value in the graph, create a rectangular ('record') node for it\n",
    "    dot.node(name = uid, label = \"{ data %.4f | grad %.4f }\" % (n.data, n.grad), shape='record')\n",
    "    if n._op:\n",
    "      # if this value is a result of some operation, create an op node for it\n",
    "      dot.node(name = uid + n._op, label = n._op)\n",
    "      # and connect this node to it\n",
    "      dot.edge(uid + n._op, uid)\n",
    "\n",
    "  for n1, n2 in edges:\n",
    "    # connect n1 to the op node of n2\n",
    "    dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n",
    "\n",
    "  return dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = Linear(1, 50)\n",
    "l2 = Linear(50, 25)\n",
    "l3 = Linear(25, 1, nonlinear=False)\n",
    "\n",
    "model = Model([l1, l2, l3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.uniform(0, 100, 1000)\n",
    "Y = X**7+2*X**3\n",
    "\n",
    "X_norm = (X - np.mean(X)) / np.std(X)\n",
    "Y_norm = (Y - np.mean(Y)) / np.std(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: -0.006566: 100%|โโโโโโโโโโ| 1000/1000 [00:52<00:00, 19.10it/s]\n"
     ]
    }
   ],
   "source": [
    "for _ in (t:=trange(1000)):\n",
    "    ix = np.random.randint(0, X.shape[0], (1,))\n",
    "    logits = model(X_norm[ix])\n",
    "\n",
    "    loss = (logits - Y_norm[ix])**2\n",
    "\n",
    "    model.zero_grad()\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    loss.step()\n",
    "    \n",
    "    t.set_description(f'loss: {loss.data[0]:.4}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data: [6.98998392e+12], grad: 0, op: +"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_value = 60\n",
    "normalized_input = (input_value - np.mean(X)) / np.std(X)\n",
    "\n",
    "# Making a prediction\n",
    "normalized_output = model([normalized_input])\n",
    "\n",
    "# Denormalizing the output\n",
    "denormalized_output = (normalized_output * np.std(Y)) + np.mean(Y)\n",
    "\n",
    "denormalized_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1772cdb50>"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X, np.array([model([x]).data for x in X_norm]) * np.std(Y) + np.mean(Y), c='r', label='prediction')\n",
    "plt.scatter(X, Y, c='b', label='actual')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cross_entropy_loss(): argument 'input' (position 1) must be Tensor, not Value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[277], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m ix \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], (\u001b[38;5;241m32\u001b[39m,))\n\u001b[1;32m      3\u001b[0m logits \u001b[38;5;241m=\u001b[39m model(X[ix])\n\u001b[0;32m----> 4\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m[\u001b[49m\u001b[43mix\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m optim\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m      8\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/functional.py:3053\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3051\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3052\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3053\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: cross_entropy_loss(): argument 'input' (position 1) must be Tensor, not Value"
     ]
    }
   ],
   "source": [
    "for _ in (t:=trange(10000)):\n",
    "    ix = torch.randint(0, X.shape[0], (32,))\n",
    "    logits = model(X[ix])\n",
    "    loss = F.cross_entropy(logits, Y[ix])\n",
    "\n",
    "    optim.zero_grad()\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    optim.step()\n",
    "    t.set_description(f'loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tomadoy.\n",
      "opnicting.\n",
      "intransmet.\n",
      "quarter.\n",
      "backn.\n",
      "maniah.\n",
      "intenvitica.\n",
      "simouncy.\n",
      "inamy.\n",
      "nutout.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7m/w8k84tcn19d4k10r234_g4lr0000gn/T/ipykernel_34045/2618988002.py:6: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = F.softmax(out)\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    word = ''\n",
    "    context = [0] * context_len\n",
    "    while True:\n",
    "        out = model(torch.tensor([context]))\n",
    "        probs = F.softmax(out)\n",
    "        ix = torch.multinomial(probs, num_samples=1)[0].item()\n",
    "        word += itos[ix]\n",
    "        context = context[1:] + [ix]\n",
    "        if ix == 0:\n",
    "            print(word)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = torch.randn(feature_count, 27)\n",
    "W1 = torch.randn(feature_count*context_len, 200)\n",
    "b1 = torch.randn(200)\n",
    "W2 = torch.randn(200, 27)\n",
    "b2 = torch.randn(27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_reg = 0.01\n",
    "batch_size = 16\n",
    "\n",
    "for _ in (t:=trange(5)):\n",
    "    batch_loss = 0  # Initialize loss for the batch\n",
    "\n",
    "    # Accumulate gradients over the batch\n",
    "    for _ in range(batch_size):\n",
    "        ix = int(numpy.random.randint(0, X.shape[0], (1,)))\n",
    "\n",
    "        input = [[0.] * 27 for _ in range(X.shape[1])]\n",
    "\n",
    "        for i in range(X.shape[1]):\n",
    "            input[i][int(X[ix][i])] = 1.0\n",
    "\n",
    "        out = model(list(np.array(input).flatten()))\n",
    "\n",
    "        expected = Y[ix]\n",
    "\n",
    "        maxVal = max([num.data for num in out])\n",
    "\n",
    "        exp = [(2**(num-maxVal)) for num in out]\n",
    "\n",
    "        count = sum([num.data for num in exp])\n",
    "\n",
    "        prob = [val/count for val in exp]\n",
    "\n",
    "        loss = prob[int(Y[ix])].log()*-1\n",
    "        batch_loss += loss.data  # Accumulate loss\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "    # Update parameters after processing the batch\n",
    "    for p in model.parameters():\n",
    "        p.data -= p.grad * 0.1 / batch_size + lambda_reg * p.data**2 \n",
    "        p.grad = 0\n",
    "    \n",
    "    t.set_description(f'Average batch loss: {batch_loss / batch_size:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix = int(numpy.random.randint(0, X.shape[0], (1,)))\n",
    "\n",
    "input = [[0.] * 27 for _ in range(X.shape[1])]\n",
    "\n",
    "for i in range(X.shape[1]):\n",
    "    input[i][int(X[ix][i])] = 1.0\n",
    "\n",
    "out = model(list(np.array(input).flatten()))\n",
    "expected = Y[ix]\n",
    "\n",
    "maxVal = max([num.data for num in out])\n",
    "\n",
    "exp = [(2**(num-maxVal)) for num in out]\n",
    "\n",
    "\n",
    "count = sum([num.data for num in exp])\n",
    "\n",
    "prob = [val/count for val in exp]\n",
    "\n",
    "loss = prob[int(Y[ix])].log()*-1\n",
    "\n",
    "predict = (0, 0)\n",
    "for i, p in enumerate(prob):\n",
    "    if p.data > predict[1]:\n",
    "        predict = (i, p.data)\n",
    "\n",
    "print(f'input: {list(np.array(input).flatten())} predicted: {predict}, actual: {expected}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(10, [1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minRange = -100\n",
    "maxRange = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.linspace(minRange, maxRange, 100)\n",
    "Y = np.linspace(minRange, maxRange, 100)\n",
    "A = np.linspace(minRange, maxRange, 100)\n",
    "B = np.linspace(minRange, maxRange, 100)\n",
    "C = np.linspace(minRange, maxRange, 100)\n",
    "D = np.linspace(minRange, maxRange, 100)\n",
    "E = np.linspace(minRange, maxRange, 100)\n",
    "F = np.linspace(minRange, maxRange, 100)\n",
    "G = np.linspace(minRange, maxRange, 100)\n",
    "H = np.linspace(minRange, maxRange, 100)\n",
    "\n",
    "random.shuffle(X)\n",
    "random.shuffle(Y)\n",
    "random.shuffle(A)\n",
    "random.shuffle(B)\n",
    "random.shuffle(C)\n",
    "random.shuffle(D)\n",
    "random.shuffle(E)\n",
    "random.shuffle(F)\n",
    "random.shuffle(G)\n",
    "random.shuffle(H)\n",
    "\n",
    "\n",
    "def Z(x, y, a, b, c, d, e, f, g, h):\n",
    "    return x + 33*y + 2*a + 3*b + 4*c + 5*d + 6*e + 33*f - 10*g + 0.5*h + 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for _ in (t:=trange(100)):\n",
    "    loss_avg = 0\n",
    "    random.shuffle(X)\n",
    "    random.shuffle(Y)\n",
    "    random.shuffle(A)\n",
    "    random.shuffle(B)\n",
    "    random.shuffle(C)\n",
    "    random.shuffle(D)\n",
    "    random.shuffle(E)\n",
    "    random.shuffle(F)\n",
    "    random.shuffle(G)\n",
    "    random.shuffle(H)\n",
    "    for x in X[:2]:\n",
    "        for y in Y[:2]:\n",
    "            for a in A[:2]:\n",
    "                for b in B[:2]:\n",
    "                    for c in C[:2]:\n",
    "                        for d in D[:2]:\n",
    "                            for e in E[:2]:\n",
    "                                for f in F[:2]:\n",
    "                                    for g in G[:2]:\n",
    "                                        for h in H[:2]:\n",
    "                                            out = model([x, y, a, b, c, d, e, f, g, h])\n",
    "\n",
    "                                            loss = (out-Z(x, y, a, b, c, d, e, f, g, h))**2\n",
    "\n",
    "                                            loss_avg += loss.data\n",
    "\n",
    "                                            loss.backward()\n",
    "\n",
    "                                            for p in model.parameters():\n",
    "                                                p.data -= 0.00001 * p.grad\n",
    "                                                p.grad = 0\n",
    "\n",
    "    t.set_description(f'{loss_avg / (X.shape[0]*Y.shape[0])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix = np.random.randint(-100, 100, (1,))\n",
    "iy = np.random.randint(-100, 100, (1,))\n",
    "ia = np.random.randint(-100, 100, (1,))\n",
    "ib = np.random.randint(-100, 100, (1,))\n",
    "ic = np.random.randint(-100, 100, (1,))\n",
    "id = np.random.randint(-100, 100, (1,))\n",
    "ie = np.random.randint(-100, 100, (1,))\n",
    "iff = np.random.randint(-100, 100, (1,))\n",
    "ig = np.random.randint(-100, 100, (1,))\n",
    "ih = np.random.randint(-100, 100, (1,))\n",
    "\n",
    "out = model([ix, iy, ia, ib, ic, id, ie, iff, ig, ih])\n",
    "\n",
    "print(f'diff: {out-Z(ix, iy, ia, ib, ic, id, ie, iff, ig, ih)}')\n",
    "\n",
    "# loss = (out-z)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.linspace(-5, 5, 100)\n",
    "Y = np.linspace(-5, 5, 100)\n",
    "\n",
    "X, Y = np.meshgrid(X, Y)\n",
    "\n",
    "Z_predicted = np.array([model([x, y]).data for x, y in zip(X.flatten(), Y.flatten())])\n",
    "\n",
    "# Reshape Z_predicted to match the shape of X and Y\n",
    "Z_predicted = Z_predicted.reshape(X.shape)\n",
    "\n",
    "print(Z_predicted.shape)\n",
    "\n",
    "Z = X + 33*Y + 8\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot_surface(X, Y, Z_predicted, color='red', label='Predicted Z')\n",
    "ax.plot_surface(X, Y, Z, alpha=0.5, color='blue', label='Actual Z')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(1, [1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.uniform(-5, 5, 1000)\n",
    "Y = X**3 + X**2 + X + 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in (t:=trange(100)):\n",
    "    loss_avg = 0\n",
    "    count = 0\n",
    "    for x, y in zip(X, Y):  \n",
    "        out = model([x**3 + x**2 + x])\n",
    "\n",
    "        loss = (out - y)**2\n",
    "\n",
    "        loss_avg += loss.data\n",
    "        count += 1\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        for p in model.parameters():\n",
    "            p.data -= 0.00001 * p.grad\n",
    "            p.grad = 0\n",
    "\n",
    "    t.set_description(f'loss: {loss_avg / max(count, 1)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_predicted = np.array([model([x**3 + x**2 + x]).data for x in X])\n",
    "\n",
    "plt.scatter(X, Y, label='Actual')\n",
    "plt.scatter(X, Y_predicted, label='Predicted')\n",
    "\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bottleneck effect\n",
    "\n",
    "X = np.random.uniform(-1, 1, 100)\n",
    "Y = np.random.uniform(-1, 1, 100)\n",
    "A = np.random.uniform(-1, 1, 100)\n",
    "B = np.random.uniform(-1, 1, 100)\n",
    "C = np.random.uniform(-1, 1, 100)\n",
    "D = np.random.uniform(-1, 1, 100)\n",
    "E = np.random.uniform(-1, 1, 100)\n",
    "F = np.random.uniform(-1, 1, 100)\n",
    "G = np.random.uniform(-1, 1, 100)\n",
    "H = np.random.uniform(-1, 1, 100)\n",
    "\n",
    "def Z(x, y, a, b, c, d, e, f, g, h):\n",
    "    return [[x+y+a+b+c+d+e+f+g+h],[2*x+2*y+2*a+2*b+3*c+8*d+9*e+f+g+h],[8*x+8*y+8*a+8*b+3*c+8*d+9*e+f+g+h],[11*x+11*y+11*a+11*b+3*c+8*d+9*e+f+g+h], [x+y+a+b+c+d+e+f+g+h],[2*x+2*y+2*a+2*b+3*c+8*d+9*e+f+g+h],[8*x+8*y+8*a+8*b+3*c+8*d+9*e+f+g+h],[11*x+11*y+11*a+11*b+3*c+8*d+9*e+f+g+h], [x+y+a+b+c+d+e+f+g+h],[2*x+2*y+2*a+2*b+3*c+8*d+9*e+f+g+h],[8*x+8*y+8*a+8*b+3*c+8*d+9*e+f+g+h],[11*x+11*y+11*a+11*b+3*c+8*d+9*e+f+g+h]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(10, [12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "avg_loss = 0\n",
    "count = 0\n",
    "for _ in (t:=trange(10000)):\n",
    "    X = np.random.uniform(-1, 1, 100)\n",
    "    Y = np.random.uniform(-1, 1, 100)\n",
    "    A = np.random.uniform(-1, 1, 100)\n",
    "    B = np.random.uniform(-1, 1, 100)\n",
    "    C = np.random.uniform(-1, 1, 100)\n",
    "    D = np.random.uniform(-1, 1, 100)\n",
    "    E = np.random.uniform(-1, 1, 100)\n",
    "    F = np.random.uniform(-1, 1, 100)\n",
    "    G = np.random.uniform(-1, 1, 100)\n",
    "    H = np.random.uniform(-1, 1, 100)\n",
    "    i += 1\n",
    "\n",
    "    for x in X[:1]:\n",
    "        for y in Y[:1]:\n",
    "            for a in A[:1]:\n",
    "                for b in B[:1]:\n",
    "                    for c in C[:1]:\n",
    "                        for d in D[:1]:\n",
    "                            for e in E[:1]:\n",
    "                                for f in F[:1]:\n",
    "                                    for g in G[:1]:\n",
    "                                        for h in H[:1]:\n",
    "                                            out = model([x, y, a, b, c, d, e, f, g, h])\n",
    "                                            z = Z(x, y, a, b, c, d, e, f, g, h)\n",
    "                                            loss = (out[0]-z[0][0])**2 + (out[1]-z[1][0])**2 + (out[2]-z[2][0])**2 + (out[3]-z[3][0])**2 + (out[4]-z[4][0])**2 + (out[5]-z[5][0])**2 + (out[6]-z[6][0])**2 + (out[7]-z[7][0])**2 + (out[8]-z[8][0])**2 + (out[9]-z[9][0])**2 + (out[10]-z[10][0])**2 + (out[11]-z[11][0])**2\n",
    "\n",
    "                                            avg_loss += loss.data\n",
    "                                            count += 1\n",
    "\n",
    "                                            loss.backward()\n",
    "\n",
    "                                            for p in model.parameters():\n",
    "                                                p.data -= 0.0001 * p.grad\n",
    "                                                p.grad = 0\n",
    "\n",
    "print(f'loss: {avg_loss/(max(count, 1))}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottleneck_model = MLP(10, [2, 12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "avg_loss = 0\n",
    "count = 0\n",
    "for _ in (t:=trange(10000)):\n",
    "    X = np.random.uniform(-1, 1, 100)\n",
    "    Y = np.random.uniform(-1, 1, 100)\n",
    "    A = np.random.uniform(-1, 1, 100)\n",
    "    B = np.random.uniform(-1, 1, 100)\n",
    "    C = np.random.uniform(-1, 1, 100)\n",
    "    D = np.random.uniform(-1, 1, 100)\n",
    "    E = np.random.uniform(-1, 1, 100)\n",
    "    F = np.random.uniform(-1, 1, 100)\n",
    "    G = np.random.uniform(-1, 1, 100)\n",
    "    H = np.random.uniform(-1, 1, 100)\n",
    "    i += 1\n",
    "\n",
    "    for x in X[:1]:\n",
    "        for y in Y[:1]:\n",
    "            for a in A[:1]:\n",
    "                for b in B[:1]:\n",
    "                    for c in C[:1]:\n",
    "                        for d in D[:1]:\n",
    "                            for e in E[:1]:\n",
    "                                for f in F[:1]:\n",
    "                                    for g in G[:1]:\n",
    "                                        for h in H[:1]:\n",
    "                                            out = bottleneck_model([x, y, a, b, c, d, e, f, g, h])\n",
    "                                            z = Z(x, y, a, b, c, d, e, f, g, h)\n",
    "                                            loss = (out[0]-z[0][0])**2 + (out[1]-z[1][0])**2 + (out[2]-z[2][0])**2 + (out[3]-z[3][0])**2 + (out[4]-z[4][0])**2 + (out[5]-z[5][0])**2 + (out[6]-z[6][0])**2 + (out[7]-z[7][0])**2 + (out[8]-z[8][0])**2 + (out[9]-z[9][0])**2 + (out[10]-z[10][0])**2 + (out[11]-z[11][0])**2\n",
    "\n",
    "                                            avg_loss += loss.data\n",
    "                                            count += 1\n",
    "\n",
    "                                            loss.backward()\n",
    "\n",
    "                                            for p in bottleneck_model.parameters():\n",
    "                                                p.data -= 0.0001 * p.grad\n",
    "                                                p.grad = 0\n",
    "\n",
    "print(f'loss: {avg_loss/(max(count, 1))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(2, [3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_loss = 0\n",
    "count = 10_000\n",
    "for _ in (t:=trange(count)):\n",
    "    ix = np.random.randint(0, X.shape[0], (1,)).item()\n",
    "    iy = np.random.randint(0, Y.shape[0], (1,)).item()\n",
    "\n",
    "    k = Z(X[ix], Y[iy])\n",
    "    \n",
    "    out = model([X[ix], Y[iy]])\n",
    "\n",
    "    exp = [(math.e**(v)) for v in out]\n",
    "\n",
    "    countV = sum([v.data for v in exp])\n",
    "\n",
    "    prob = [val/countV for val in exp]\n",
    "\n",
    "    loss = prob[k].log()\n",
    "\n",
    "    avg_loss += loss.data\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    for p in model.parameters():\n",
    "        p.data += 0.0000001 * p.grad\n",
    "        p.grad = 0\n",
    "\n",
    "    t.set_description(f'prob: {prob[k]}')\n",
    "print(f'avg loss: {-avg_loss/count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "trials = 10_000\n",
    "for _ in range(trials):\n",
    "    ix = np.random.randint(0, X.shape[0], (1,)).item()\n",
    "    iy = np.random.randint(0, Y.shape[0], (1,)).item()\n",
    "\n",
    "    k = Z(X[ix], Y[iy], A[ia])\n",
    "        \n",
    "    out = model([X[ix], Y[iy], A[ia]])\n",
    "\n",
    "    exp = [(math.e**(v)) for v in out]\n",
    "\n",
    "    countV = sum([v.data for v in exp])\n",
    "\n",
    "    prob = [val/countV for val in exp]\n",
    "\n",
    "    if prob[k].data == max([p.data for p in prob]):\n",
    "        correct += 1\n",
    "    \n",
    "print(f'accuracy: {correct/trials*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = {}\n",
    "for _ in range(100_000):\n",
    "    ix = np.random.randint(0, X.shape[0], (1,)).item()\n",
    "    iy = np.random.randint(0, Y.shape[0], (1,)).item()\n",
    "\n",
    "    k = Z(X[ix], Y[iy])\n",
    "    out[k] = out.get(k, 0) + 1\n",
    "\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi class \n",
    "X = np.random.uniform(-10, 10, 1000)\n",
    "Y = np.random.uniform(-10, 10, 1000)\n",
    "A = np.random.uniform(-10, 10, 1000)\n",
    "B = np.random.uniform(-10, 10, 1000)\n",
    "\n",
    "def Z(x, y, a, b):\n",
    "    value = np.sin(x) * np.cos(y) + np.tanh(a * b)\n",
    "\n",
    "    if value < -0.5:\n",
    "        return 0\n",
    "    elif -0.5 <= value < 0.5:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = torch.randn(4, 20) * 0.01\n",
    "b1 = torch.randn(20) * 0.0\n",
    "W2 = torch.randn(20, 3) * 0.01\n",
    "b2 = torch.randn(3) * 0.0\n",
    "\n",
    "parameters = [W1, b1, W2, b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.Adam(parameters)\n",
    "for _ in (t:=trange(10_000)):\n",
    "    ix = np.random.randint(0, X.shape[0], (1,)).item()\n",
    "    iy = np.random.randint(0, Y.shape[0], (1,)).item()\n",
    "    ia = np.random.randint(0, A.shape[0], (1,)).item()\n",
    "    ib = np.random.randint(0, B.shape[0], (1,)).item()\n",
    "\n",
    "    k = Z(X[ix], Y[iy], A[ia], B[ib])\n",
    "    input = torch.tensor([X[ix], Y[iy], A[ia], B[ib]]).float()\n",
    "    hpreact = input @ W1 + b1\n",
    "    h = torch.tanh(hpreact)\n",
    "    logits = h @ W2 + b2\n",
    "\n",
    "    optim.zero_grad()\n",
    "    \n",
    "    loss = F.cross_entropy(logits, torch.tensor(k))\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    optim.step()\n",
    "\n",
    "    t.set_description(f'loss: {loss.data}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "trials = 10_000\n",
    "for _ in range(trials):\n",
    "    ix = np.random.randint(0, X.shape[0], (1,)).item()\n",
    "    iy = np.random.randint(0, Y.shape[0], (1,)).item()\n",
    "    ia = np.random.randint(0, A.shape[0], (1,)).item()\n",
    "    ib = np.random.randint(0, B.shape[0], (1,)).item()\n",
    "\n",
    "    k = Z(X[ix], Y[iy], A[ia], B[ib])\n",
    "    input = torch.tensor([X[ix], Y[iy], A[ia], B[ib]]).float()\n",
    "    hpreact = input @ W1 + b1\n",
    "    h = torch.tanh(hpreact)\n",
    "    logits = h @ W2 + b2\n",
    "\n",
    "    probs = F.softmax(logits)\n",
    "\n",
    "    out = torch.multinomial(probs, num_samples=1)[0].item()\n",
    "\n",
    "    if torch.argmax(probs) == out:\n",
    "        correct += 1\n",
    "    \n",
    "print(f'accuracy: {correct/trials*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def fetch(url):\n",
    "    import requests, gzip, os, hashlib, numpy as np\n",
    "    fp = os.path.join('tmp/' + hashlib.md5(url.encode('utf-8')).hexdigest() + '.gz')\n",
    "    if os.path.isfile(fp):\n",
    "        with open(fp, 'rb') as f:\n",
    "            dat = f.read()\n",
    "    else:\n",
    "        with open(fp, 'wb') as f:\n",
    "            dat = requests.get(url).content\n",
    "            f.write(dat)\n",
    "    return np.frombuffer(gzip.decompress(dat), dtype=np.uint8).copy()\n",
    "\n",
    "X_train = torch.tensor(fetch(\"http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\")[16:].reshape((-1, 28, 28)))\n",
    "Y_train = torch.tensor(fetch(\"http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\")[8:])\n",
    "X_test = torch.tensor(fetch(\"http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\")[16:].reshape((-1, 28, 28)))\n",
    "Y_test = torch.tensor(fetch(\"http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\")[8:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = torch.randn(28*28, 200) * 0.01\n",
    "b1 = torch.randn(200) * 0.0\n",
    "W2 = torch.randn(200, 100) * 0.01\n",
    "b2 = torch.randn(100) * 0.0\n",
    "W3 = torch.randn(100, 10) * 0.01\n",
    "b3 = torch.randn(10) * 0.0\n",
    "\n",
    "parameters = [W1, b1, W2, b2, W3, b3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.Adam(parameters)\n",
    "for _ in (t:=trange(1000)):\n",
    "    ix = torch.randint(0, X_train.shape[0], (32,))\n",
    "    l1 = torch.tanh(X_train[ix].flatten(1).float() @ W1 + b1)\n",
    "    l2 = torch.tanh(l1 @ W2 + b2)\n",
    "    logits = l2 @ W3 + b3\n",
    "\n",
    "    loss = F.cross_entropy(logits, Y_train[ix])\n",
    "\n",
    "    optim.zero_grad()\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    optim.step()\n",
    "\n",
    "    t.set_description(f'loss: {loss.data}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "trials = 100000\n",
    "for _ in range(trials):\n",
    "    ix = torch.randint(0, X_train.shape[0], (1,))\n",
    "    l1 = torch.tanh(X_train[ix].flatten(1).float() @ W1 + b1)\n",
    "    l2 = torch.tanh(l1 @ W2 + b2)\n",
    "    logits = l2 @ W3 + b3\n",
    "\n",
    "    prob = F.softmax(logits)\n",
    "\n",
    "    if torch.argmax(prob).item() == Y_train[ix].item():\n",
    "        correct += 1\n",
    "\n",
    "print(f'accuracy: {correct/trials}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
