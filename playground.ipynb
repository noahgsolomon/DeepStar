{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('words.txt', 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(set('.'.join(words)))\n",
    "\n",
    "stoi = {c: i for i, c in enumerate(chars)}\n",
    "itos = {i:c for i, c in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = {}\n",
    "for word in words:\n",
    "    word = '.' + word + '.'\n",
    "    for ch1, ch2 in zip(word, word[1:]):\n",
    "        bigram[(ch1, ch2)] = bigram.get((ch1, ch2), 0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = torch.ones(27, 27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (ch1, ch2), count in bigram.items():\n",
    "    N[stoi[ch1]][stoi[ch2]] = count\n",
    "N = N + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = N / N.sum(dim=1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.leagronliont.',\n",
       " '.tsererty.',\n",
       " '.btr.',\n",
       " '.cte.',\n",
       " '.rod.',\n",
       " '.m.',\n",
       " '.didingaren.',\n",
       " '.mat.',\n",
       " '.caboma.',\n",
       " '.s.']"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = []\n",
    "for _ in range(10):\n",
    "    word = '.'\n",
    "    while True:\n",
    "        ix = torch.multinomial(P[stoi[word[-1]]], num_samples=1)[0].item()\n",
    "        word += itos[ix]\n",
    "        if ix == 0:\n",
    "            res.append(word)\n",
    "            break\n",
    "    \n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.5044)\n"
     ]
    }
   ],
   "source": [
    "loss = 0\n",
    "count = 0\n",
    "for word in words:\n",
    "    word = '.' + word + '.'\n",
    "    for ch1, ch2 in zip(word, word[1:]):\n",
    "        ix = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        prob = P[ix][ix2]\n",
    "        loss += -torch.log(P[ix][ix2])\n",
    "        count += 1\n",
    "\n",
    "loss /= count\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_len = 4\n",
    "ch_features = 20\n",
    "W_size = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_len = 4\n",
    "def build_dataset(words):\n",
    "    X, Y = [], []\n",
    "    for word in words:\n",
    "        word += '.'\n",
    "        context = [0] * context_len\n",
    "        for ch in word:\n",
    "            X.append(context)\n",
    "            Y.append(stoi[ch])\n",
    "            context = context[1:] + [stoi[ch]]\n",
    "    \n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.shuffle(words)\n",
    "\n",
    "n = int(0.8 * len(words))\n",
    "\n",
    "Xtrain, Ytrain = build_dataset(words[:n])\n",
    "\n",
    "Xval, Yval = build_dataset(words[n:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = torch.randn(27, ch_features) * 0.01\n",
    "W1 = torch.randn(ch_features * context_len, W_size) * 0.01\n",
    "b1 = torch.randn(W_size) * 0\n",
    "W2 = torch.randn(W_size, 27) * 0.01\n",
    "b2 = torch.randn(27) * 0\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "    p.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.5617, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.6506, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9037, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8653, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8983, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.6103, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8715, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.7970, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1799, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.5446, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9228, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9547, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8468, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3144, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8011, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1995, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8556, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.6176, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.6259, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.6298, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.5893, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.0436, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9282, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.7050, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8653, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8335, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9095, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9325, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8158, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.4423, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5216, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.0236, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.0101, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9116, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9871, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.6141, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1523, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9113, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.6278, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1953, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.6810, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.7594, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.5985, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8151, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.4647, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9748, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.5928, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1817, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8149, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.6188, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.5972, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8546, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2212, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9362, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.0353, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.6025, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1980, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.7489, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.0498, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8602, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.6616, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.0226, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.0817, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8478, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3693, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1152, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.7551, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.6672, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.0681, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.7361, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2075, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.5296, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.0699, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.0145, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.5921, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9063, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.6133, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.4746, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.6423, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.2751, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.6572, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.7159, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1635, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1305, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1600, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.7750, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8446, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9655, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.7515, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.0979, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8099, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.6218, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.0502, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.7946, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3485, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8102, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.7076, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9202, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3719, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3697, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for i in range(10000):\n",
    "    ix = torch.randint(0, Xtrain.shape[0], (32,))\n",
    "    emb = C[Xtrain[ix]].flatten(1)\n",
    "    hpreact = emb @ W1 + b1\n",
    "    h = torch.tanh(hpreact)\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, Ytrain[ix])\n",
    "    if i % 100 == 0:\n",
    "        print(loss)\n",
    "    loss.backward()\n",
    "    for p in parameters:\n",
    "        p.data -= p.grad * 0.01\n",
    "        p.grad = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.7988, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = C[Xval].flatten(1)\n",
    "hpreact = emb @ W1 + b1\n",
    "h = torch.tanh(hpreact)\n",
    "logits = h @ W2 + b2\n",
    "loss = F.cross_entropy(logits, Yval)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'context_len' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m res \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m----> 3\u001b[0m     context \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[43mcontext_len\u001b[49m\n\u001b[1;32m      4\u001b[0m     word \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'context_len' is not defined"
     ]
    }
   ],
   "source": [
    "res = []\n",
    "for _ in range(10):\n",
    "    context = [0] * context_len\n",
    "    word = ''\n",
    "    while True:\n",
    "        emb = C[torch.tensor([context])].flatten(1)\n",
    "        hpreact = emb @ W1 + b1\n",
    "        h = torch.tanh(hpreact)\n",
    "        logits = h @ W2 + b2\n",
    "        ix = torch.multinomial(F.softmax(logits), num_samples=1)[0].item()\n",
    "        word += itos[ix]\n",
    "        context = context[1:] + [ix]\n",
    "        if ix == 0:\n",
    "            res.append(word)\n",
    "            break\n",
    "\n",
    "res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
